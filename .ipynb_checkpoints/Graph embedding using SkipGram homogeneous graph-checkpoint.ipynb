{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embedding using SkipGram\n",
    "\n",
    "This is a duplicate of the other Skipgram NB, except we are embedding without CORD19 to investigate the effect of publications on the ranking of ChEMBL antivirals. Specifically we want to know whether this ranking is simply ranking the antiviral drugs by regurgitating the knowledge it extracts from publications.\n",
    "\n",
    "The SkipGram model predicts the context using the central word.\n",
    "\n",
    "In our implementatation, as for both the Binary SkipGram model and the CBOW model, since the batches walks are lazily generated, the memory requirements are minimal and the method can scale to very big graphs. It can also run on graphs like Monarch (150M edges and 50M nodes) provided that you use a GPU ([or better still a TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#console)) that is able to fit an embedding model that big, but that is just related to the shear number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jtr4v/kg_covid_19_drug_analyses', '/home/jtr4v/anaconda3/lib/python38.zip', '/home/jtr4v/anaconda3/lib/python3.8', '/home/jtr4v/anaconda3/lib/python3.8/lib-dynload', '', '/home/jtr4v/anaconda3/lib/python3.8/site-packages', '/home/jtr4v/anaconda3/lib/python3.8/site-packages/IPython/extensions', '/home/jtr4v/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graphs\n",
    "We load the ppi graph from the repository as an undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.makedirs(\"graph_no_cord_19\", exist_ok=True)\n",
    "if not os.path.exists(\"graph_no_cord_19/merged-kg.tar.gz\"):\n",
    "    with urllib.request.urlopen(\"https://zenodo.org/record/4012578/files/merged-kg.tar.gz\") as response, \\\n",
    "        open(\"graph_no_cord_19/merged-kg.tar.gz\", 'wb') as out_file:  # type: ignore\n",
    "            data = response.read()  # a `bytes` object\n",
    "            out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"tar -xvzf graph_no_cord_19/merged-kg.tar.gz -C graph_no_cord_19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "graph = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"graph_no_cord_19/merged-kg_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    default_edge_type=\"biolink:association\",\n",
    "    node_path=\"graph_no_cord_19/merged-kg_nodes.tsv\",\n",
    "    nodes_column=\"id\",\n",
    "    node_types_column=\"category\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first thing, we print a short report showing all the avalable graph details, including the number of edges, nodes, trap nodes and both the connected components and the strongly connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degrees_median': '2',\n",
       " 'degrees_max': '27848',\n",
       " 'unique_edge_types_number': '0',\n",
       " 'density': '0.0002932254882749931',\n",
       " 'degrees_mean': '60.167817615610666',\n",
       " 'connected_components_number': '9419',\n",
       " 'selfloops_rate': '0.000038473952931370974',\n",
       " 'traps_rate': '0.042228536061171676',\n",
       " 'nodes_number': '205193',\n",
       " 'degrees_min': '0',\n",
       " 'singleton_nodes': '8665',\n",
       " 'strongly_connected_components_number': '9419',\n",
       " 'degrees_mode': '1',\n",
       " 'is_directed': 'false',\n",
       " 'bidirectional_rate': '1',\n",
       " 'unique_node_types_number': '28',\n",
       " 'edges_number': '12346015'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are check that are not necessary, but are offered as sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert graph > training\n",
    "assert graph > validation\n",
    "assert (training + validation).contains(graph)\n",
    "assert graph.contains(training + validation)\n",
    "assert not training.overlaps(validation)\n",
    "assert not validation.overlaps(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considered parameters\n",
    "We are going to use the following parameters:\n",
    "\n",
    "- **Walk lengths:** $100$ nodes.\n",
    "- **Batch size:** $2^{7} = 128$ walks per batch.\n",
    "- **Walk iterations:** $20$ iterations on the graph.\n",
    "- **Window size:** $4$ nodes, meaning $4$ on the left and $4$ on the right of the center nodes. Consider that the first *window_size* values on the left and the right of the walks will be trimmed.\n",
    "- **Return weight, inverse of $p$:** $1.0$.\n",
    "- **Explore weight, inverse of $q$:** $1.0$.\n",
    "- **Embedding size:** $100$.\n",
    "- **Negative samples:** For the porpose of the [NCE function negative samples](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss), we are going to use $10$. These are the number of negative classes to randomly sample per batch. This single sample of negative classes is evaluated for each element in the batch.\n",
    "- **Optimizer:** [Nadam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam).\n",
    "- **Early stopping parameters:** We are going to use an Early Stopping criterion on the *validation loss*, with patience $5$ and delta $0.0001$.\n",
    "- **Epochs:** The model will be trained up to $1000$ epochs.\n",
    "- **Learning rate:** since tipically the loss function is quite convex for the embedding problem, we can use a relatively higher learning rate. We are going to us $0.1$ for this example, to get to a faster convergence: this might lead to skipping some better minima that might be identified with a lower learning rate, such as the default one which is $0.0001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length=100\n",
    "batch_size=2**7\n",
    "iterations=20\n",
    "window_size=4\n",
    "p=1.0\n",
    "q=1.0\n",
    "embedding_size=100\n",
    "negatives_samples=30\n",
    "patience=5\n",
    "delta=0.0001\n",
    "epochs=1000\n",
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation Keras sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embiggen import Node2VecSequence\n",
    "\n",
    "graph_sequence = Node2VecSequence(\n",
    "    graph,\n",
    "    walk_length=walk_length,\n",
    "    batch_size=batch_size,\n",
    "    iterations=iterations,\n",
    "    window_size=window_size,\n",
    "    return_weight=1/p,\n",
    "    explore_weight=1/q\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the SkipGram model\n",
    "We are going to setup the model to use, if available, multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SkipGram\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_embedding (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       20519300    words_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "noise_contrastive_estimation_1  (None, 205193)       20724493    flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,243,793\n",
      "Trainable params: 41,243,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from embiggen import SkipGram\n",
    "\n",
    "strategy = MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = SkipGram(\n",
    "        vocabulary_size=training.get_nodes_number(),\n",
    "        embedding_size=embedding_size,\n",
    "        window_size=window_size,\n",
    "        negatives_samples=negatives_samples,\n",
    "        optimizer=Nadam(learning_rate=learning_rate)\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the SkipGram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1536/1536 [==============================] - 296s 193ms/step - loss: 74.8272\n",
      "Epoch 2/1000\n",
      "1536/1536 [==============================] - 303s 197ms/step - loss: 47.4429\n",
      "Epoch 3/1000\n",
      " 301/1536 [====>.........................] - ETA: 4:02 - loss: 47.2341"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    graph_sequence,\n",
    "    steps_per_epoch=graph_sequence.steps_per_epoch,\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            \"val_loss\",\n",
    "            min_delta=delta,\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights\n",
    "We save the obtained model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{model.name}_no_cord19_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training history\n",
    "We can visualize the performance of the model during the training process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some hickups in the plot of the history if the model is reloaded from stored weights: [this is a known Keras issue](https://github.com/keras-team/keras/issues/4875) and is not related to either the holdouts used or the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the obtained embeddings\n",
    "Finally we save our hard earned model embeddings. In another notebook we will show how to do link prediction on the obtained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(f\"{model.name}_no_cord19_embedding.npy\", model.embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
