{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embedding using SkipGram\n",
    "\n",
    "This is an embedding of the whole graph, 80/20 training and validation split and all sources\n",
    "\n",
    "kg-covid-19:\n",
    "version 20201012\n",
    "\n",
    "Name: ensmallen-graph\n",
    "Version: 0.4.4\n",
    "\n",
    "Name: embiggen\n",
    "Version: 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import get_distribution\n",
    "assert(get_distribution(\"ensmallen-graph\").version == '0.4.4')  # identical to 0.4.3 except for addition of some methods like get_edge_id()\n",
    "assert(get_distribution(\"embiggen\").version == '0.6.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "exp_name = \"80_20_kg_covid_19_20201012_training_test_epoch_500_delta_0.0001_updated_holdouts\"\n",
    "s3_path = \"s3://kg-hub-public-data/embeddings/20201012/\"  # keep trailing slash\n",
    "\n",
    "base_dl_dir = \"downloaded_data\"\n",
    "graph_data_dir = os.path.join(base_dl_dir, \"kg-covid-19-20201012\")\n",
    "embedding_data_dir = os.path.join(base_dl_dir, \"embeddings-20201012\")\n",
    "\n",
    "# graph stuff\n",
    "graph_out_file = os.path.join(graph_data_dir + \"/kg-covid-19.tar.gz\")\n",
    "nodes_file = os.path.join(graph_data_dir, \"merged-kg_nodes.tsv\")\n",
    "edges_file = os.path.join(graph_data_dir, \"merged-kg_edges.tsv\")\n",
    "sorted_edges_file = os.path.join(graph_data_dir, \"merged-kg_edges_SORTED.tsv\")\n",
    "graph_tar_url = \"https://kg-hub.berkeleybop.io/kg-covid-19/20201012/kg-covid-19.tar.gz\"\n",
    "\n",
    "# embeddings URLs\n",
    "base_kghub_url = \"http://kg-hub.berkeleybop.io/\"\n",
    "embeddings_url = os.path.join(base_kghub_url, \"embeddings/20201012/SkipGram_80_20_kg_covid_19_20201012_training_test_epoch_500_delta_0.0001_embedding.npy\")\n",
    "embedding_file = os.path.join(embedding_data_dir, \"SkipGram_embedding.npy\")\n",
    "\n",
    "# params\n",
    "seed = 42\n",
    "train_percentage = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graphs\n",
    "We load the kg-covid-19 graph from the repository as an undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the graphs, if necessary\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "os.makedirs(graph_data_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(nodes_file) or not os.path.exists(edges_file):\n",
    "    with urllib.request.urlopen(graph_tar_url) as response, \\\n",
    "        open(graph_out_file, 'wb') as out_file:\n",
    "            data = response.read()  # a `bytes` object\n",
    "            out_file.write(data)\n",
    "    os.system(\"tar -xvzf \" + graph_out_file + \" -C \" + graph_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only need to do this once, b/c we'll load the sorted.tsv from now on once it is made below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtr4v/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0,9,10,11,12,13,14,15,16,17,18,21,22,24,25,26,29,30,31,32,33,34,37,41,43,45,48) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "edges = pd.read_csv(graph_data_dir + \"/merged-kg_edges.tsv\", \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars_cov_2_curie = 'NCBITaxon:2697049'\n",
    "chembl_prefix = 'CHEMBL.COMPOUND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_to_sars_cov_2_edges = (\n",
    "    (edges.subject.str.contains(chembl_prefix) & (edges.object == sars_cov_2_curie)) | \n",
    "    (edges.object.str.contains(chembl_prefix) & (edges.subject == sars_cov_2_curie))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges['holdout_edge_label'] = [\n",
    "    'chembl_to_sars_cov_2' if value else 'normal'\n",
    "    for value in chembl_to_sars_cov_2_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_edge_file = os.path.join(graph_data_dir, 'edges_with_holdout_column.tsv')\n",
    "edges.to_csv(new_edge_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "graph = EnsmallenGraph.from_unsorted_csv(\n",
    "    name=\"kg-covid-19\",\n",
    "    edge_path = new_edge_file,\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    edge_types_column='holdout_edge_label',\n",
    "    directed=False,\n",
    "    node_path = graph_data_dir + \"/merged-kg_nodes.tsv\",\n",
    "    nodes_column = 'id',\n",
    "    node_types_column = 'category',\n",
    "    default_node_type = 'biolink:NamedThing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The undirected graph Graph has 447766 nodes with 42 different node types:  the 5 most common are biolink:Publication (nodes number 129930), biolink:OntologyClass (nodes number 108266), biolink:Drug (nodes number 32120), biolink:ChemicalSubstance (nodes number 27157) and biolink:Disease (nodes number 24236), of which 8355 are singletons, and 15611957 unweighted edges with 2 different edge types: normal and chembl_to_sars_cov_2, of which 480 are self-loops. The graph is dense as it has a density of 0.01067 and has 9107 connected components, where the component with most nodes has 435728 nodes and the component with the least nodes has 1 nodes. The graph median node degree is 4, the mean node degree is 69.73 and the node degree mode is 1. The top 5 most central nodes are MESH:D014780 (degree 90378), MESH:D006801 (degree 78249), WD:Q30 (degree 65223), MESH:D014777 (degree 54155) and MESH:D017934 (degree 45196)."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_graph = graph.remove_components(edge_types=['chembl_to_sars_cov_2'])\n",
    "reduced_graph = reduced_graph.remove(singletons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The undirected graph Graph has 435728 nodes with 40 different node types:  the 5 most common are biolink:Publication (nodes number 129492), biolink:OntologyClass (nodes number 104951), biolink:Drug (nodes number 32016), biolink:ChemicalSubstance (nodes number 27152) and biolink:Disease (nodes number 22281) and 15608444 unweighted edges with 2 different edge types: normal and chembl_to_sars_cov_2, of which 314 are self-loops. The graph is dense as it has a density of 0.03548 and is connected, as it has a single component. The graph median node degree is 4, the mean node degree is 71.64 and the node degree mode is 1. The top 5 most central nodes are MESH:D014780 (degree 90378), MESH:D006801 (degree 78249), WD:Q30 (degree 65223), MESH:D014777 (degree 54155) and MESH:D017934 (degree 45196)."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the SkipGram model\n",
    "We are going to setup the model to use, if available, multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cache_decorator import Cache\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from embiggen import SkipGram\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "@Cache(\n",
    "    cache_path=\"{cache_dir}/SkipGram/{_hash}_{holdout_idx}.csv.gz\",\n",
    "    cache_dir=embedding_data_dir,\n",
    "    args_to_ignore=['train_graph']\n",
    ")\n",
    "def compute_skipgram_embedding(\n",
    "    train_graph: EnsmallenGraph,\n",
    "    holdout_idx: int,\n",
    "    walk_length: int = 100,\n",
    "    batch_size: int = 2**9,\n",
    "    iterations: int = 20,\n",
    "    return_weight: float = 1.0,\n",
    "    explore_weight: float = 1.0,\n",
    "    embedding_size: int = 100,\n",
    "    window_size: int = 4,\n",
    "    negative_samples: int = 7,\n",
    "    patience: int = 6,\n",
    "    delta: float = 0.1\n",
    "):\n",
    "    \"\"\"Return dataframe with node embedding obtained with SkipGram for train_graph\n",
    "    \n",
    "    Given a graph, learn embeddings and return dataframe with node embeddings\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    train_graph: EnsmallenGraph\n",
    "    holdout_idx: int, \n",
    "        an int to identify the holdout\n",
    "    walk_length: int = 100,\n",
    "        how many nodes for each walk\n",
    "    batch_size: int = 2**9,\n",
    "        how many walks for each batch\n",
    "    iterations: int = 20,\n",
    "        how many walks per node\n",
    "    return_weight: float = 1.0,\n",
    "        node2vec param, equal to 1/p\n",
    "    explore_weight: float = 1.0,\n",
    "        node2vec param, equal to 1/q\n",
    "    embedding_size: int = 100,\n",
    "        dimensions for embedding\n",
    "    window_size: int = 4,\n",
    "        SkipGram window size\n",
    "    negative_samples: int = 7,\n",
    "        how many negative samples that NCE function needs to sample\n",
    "    patience: int = 6,\n",
    "        how many epochs to wait for loss fxn to improve by [delta]\n",
    "    delta: float = 0.1\n",
    "        change in loss fxn to be considered an improvement\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pd.DataFrame containing an embedding for each node in train_graph\n",
    "    \"\"\"\n",
    "    training_sequence = Node2VecSequence(\n",
    "        train_graph,\n",
    "        walk_length=walk_length,\n",
    "        batch_size=batch_size,\n",
    "        iterations=iterations,\n",
    "        window_size=window_size,\n",
    "        return_weight=1/p,\n",
    "        explore_weight=1/q,\n",
    "        support_mirror_strategy=True\n",
    "    )\n",
    "\n",
    "    strategy = MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = SkipGram(\n",
    "            vocabulary_size=train_graph.get_nodes_number(),\n",
    "            embedding_size=embedding_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=negative_samples,\n",
    "        )\n",
    "\n",
    "    history = model.fit(\n",
    "        training_sequence,\n",
    "        steps_per_epoch=training_sequence.steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                \"loss\",\n",
    "                min_delta=delta,\n",
    "                patience=patience,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=\"loss\",\n",
    "                patience=patience//2,\n",
    "                min_delta=delta\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return model.get_embedding_dataframe(train_graph.get_nodes_reverse_mapping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/monarch-initiative/syntheticLethalityNetwork/tree/embedding_sli/sli_embedding_and_ranking/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374fa9d525834f6e9162fa21fadeb7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='computing embeddings', max=5.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "852/852 [==============================] - 422s 495ms/step - loss: 73.8198\n",
      "Epoch 2/500\n",
      "639/852 [=====================>........] - ETA: 1:45 - loss: 41.5210"
     ]
    }
   ],
   "source": [
    "# make holdouts\n",
    "from tqdm.auto import trange\n",
    "\n",
    "for i in trange(5, desc=\"computing embeddings\"):\n",
    "    pos_training, pos_validation = reduced_graph.connected_holdout(\n",
    "        train_size=train_percentage, \n",
    "        edge_types=['chembl_to_sars_cov_2'],\n",
    "        random_state=seed+i)\n",
    "    pos_training.enable_fast_walk()\n",
    "    embedding = compute_skipgram_embedding(pos_training, i)\n",
    "    \n",
    "    \n",
    "#       transformer = LinkPredictionTransformer(method=\"hadamard\")\n",
    "#       transformer.fit(embedding)\n",
    "#       train_X, train_y = transformer.transform(train_pos_sli, train_neg_sli)\n",
    "#       test_X, test_y = transformer.transform(test_pos_sli, test_neg_sli)\n",
    "#       for link_prediction_model in link_prediction_models:\n",
    "#             train_pred, test_pred = link_prediction_model(holdout_number, train_X, train_y, test_X)\n",
    "#             for run, true, predictions in (\n",
    "#                 (\"train\", train_y, train_pred),\n",
    "#                 (\"test\", test_y, test_pred)\n",
    "#             ):\n",
    "#                 results.append({\n",
    "#                     \"run\": run,\n",
    "#                     \"model\": link_prediction_model.__name__,\n",
    "#                     \"change_edge_type_weight\": change_edge_type_weight,\n",
    "#                     \"holdout_number\": holdout_number,\n",
    "#                     **get_metrics_report(true, predictions)\n",
    "#                 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the weights and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for path in glob(f\"{embedding_data_dir}/**/*.csv.gz\", recursive=True):\n",
    "    os.system(f\"s3cmd put --acl-public --cf-invalidate {path} {s3_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
