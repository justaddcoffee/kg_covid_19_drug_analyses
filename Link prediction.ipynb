{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "In this notebook we do link prediction starting from the embeddings\n",
    "previously obtained from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge embeddings\n",
    "We will compute the edge embeddings using all the 5 available methods:\n",
    "\n",
    "- Hadamart: an element-wise product\n",
    "- Mean\n",
    "- Norm L1\n",
    "- Norm L2\n",
    "- Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import AUC, Recall, Precision\n",
    "\n",
    "def build_link_prediction_model(input_shape:int):\n",
    "    model = Sequential([\n",
    "        Input(input_shape),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\n",
    "            AUC(curve=\"PR\", name=\"auprc\"),\n",
    "            AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "            Recall(),\n",
    "            Precision(),\n",
    "            \"accuracy\"\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "graph = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"/home/jtr4v/merged-kg_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_edge_type=\"biolink:association\",\n",
    "    node_path=\"/home/jtr4v/merged-kg_nodes.tsv\",\n",
    "    nodes_column=\"id\",\n",
    "    node_types_column=\"category\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degrees_max': '90378',\n",
       " 'unique_edge_types_number': '211',\n",
       " 'degrees_min': '0',\n",
       " 'singleton_nodes': '8223',\n",
       " 'density': '0.00021904928054478553',\n",
       " 'degrees_mean': '82.22343319169342',\n",
       " 'strongly_connected_components_number': '8976',\n",
       " 'degrees_median': '6',\n",
       " 'traps_rate': '0.021906677500566116',\n",
       " 'nodes_number': '375365',\n",
       " 'is_directed': 'false',\n",
       " 'edges_number': '30863799',\n",
       " 'selfloops_rate': '0.00001539019872440201',\n",
       " 'connected_components_number': '8976',\n",
       " 'bidirectional_rate': '1',\n",
       " 'unique_node_types_number': '36',\n",
       " 'degrees_mode': '1'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining holdouts and tasks data generator\n",
    "We are going to create the same edge embeddings as in the training of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train_percentage = 0.8\n",
    "\n",
    "pos_training, pos_validation = graph.connected_holdout(seed, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python /home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/run.py edges -n /home/jtr4v/merged-kg_nodes-min.tsv -e /home/jtr4v/merged-kg_edges-min.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_training, neg_validation = graph.sample_negatives(\n",
    "#    seed=seed,\n",
    "#    negatives_number=graph.get_edges_number(),\n",
    "#    allow_selfloops=False\n",
    "#).random_holdout(seed=seed, train_percentage=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "neg_validation = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"/home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/data/edges/neg_test_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")\n",
    "\n",
    "neg_training = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"/home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/data/edges/neg_train_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from embiggen import GraphTransformer, EdgeTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def task_generator(\n",
    "    pos_training:EnsmallenGraph,\n",
    "    pos_validation:EnsmallenGraph,\n",
    "    neg_training:EnsmallenGraph,\n",
    "    neg_validation:EnsmallenGraph,\n",
    "    train_percentage:float=train_percentage,\n",
    "    seed:int=seed\n",
    "):\n",
    "    \"\"\"Create new generator of tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------\n",
    "    pos_training:EnsmallenGraph,\n",
    "        The positive edges of the training graph.\n",
    "    pos_validation:EnsmallenGraph,\n",
    "        The positive edges of the validation graph.\n",
    "    neg_training:EnsmallenGraph,\n",
    "        The negative edges of the training graph.\n",
    "    neg_validation:EnsmallenGraph,\n",
    "        The negative edges of the validation graph.\n",
    "    train_percentage:float=0.8,\n",
    "    seed:int=42\n",
    "\n",
    "    \"\"\"\n",
    "    for path in tqdm(glob(\"*embedding.npy\"), desc=\"Embedding\"):\n",
    "        model_name = path.split(\"_\")[0]\n",
    "        embedding = np.load(path)\n",
    "        for method in tqdm(EdgeTransformer.methods, desc=\"Methods\", leave=False):\n",
    "            transformer = GraphTransformer(method)\n",
    "            transformer.fit(embedding)\n",
    "            train_edges = np.vstack([\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_training, neg_training)\n",
    "            ])\n",
    "            valid_edges = np.vstack([\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_validation, neg_validation)\n",
    "            ])\n",
    "            train_labels = np.concatenate([\n",
    "                np.ones(pos_training.get_edges_number()),\n",
    "                np.zeros(neg_training.get_edges_number())\n",
    "            ])\n",
    "            valid_labels = np.concatenate([\n",
    "                np.ones(pos_validation.get_edges_number()),\n",
    "                np.zeros(neg_validation.get_edges_number())\n",
    "            ])\n",
    "            train_indices = np.arange(0, train_labels.size)\n",
    "            valid_indices = np.arange(0, valid_labels.size)\n",
    "            np.random.shuffle(train_indices)\n",
    "            np.random.shuffle(valid_indices)\n",
    "            train_edges = train_edges[train_indices]\n",
    "            train_labels = train_labels[train_indices]\n",
    "            valid_edges = valid_edges[valid_indices]\n",
    "            valid_labels = valid_labels[valid_indices]\n",
    "            yield model_name, method, (train_edges, train_labels), (valid_edges, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6065733190a4e53bae5303faa17f363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Embedding', max=1.0, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "histories = {}\n",
    "strategy = MirroredStrategy()\n",
    "os.makedirs(\"classical_link_prediction\", exist_ok=True)\n",
    "\n",
    "for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "    history_path = f\"classical_link_prediction/{embedding_model}_{method}.csv\"\n",
    "    if os.path.exists(history_path):\n",
    "        histories[(embedding_model, method)] = pd.read_csv(history_path)\n",
    "        continue\n",
    "    with strategy.scope():\n",
    "        model = build_link_prediction_model(train[0].shape[1:])\n",
    "    history = pd.DataFrame(model.fit(\n",
    "        *train,\n",
    "        batch_size=2**12,\n",
    "        validation_data=valid,\n",
    "        epochs=1000,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\"val_loss\", patience=20, min_delta=0.0001),\n",
    "            ReduceLROnPlateau()\n",
    "        ]\n",
    "    ).history)\n",
    "\n",
    "    history.to_csv(history_path, index=False)\n",
    "    histories[(embedding_model, method)] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all the computer histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "\n",
    "for history in histories.values():\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of various embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we covert the histories into an homogeneous report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "\n",
    "report = []\n",
    "for (model, method), history in histories.items():\n",
    "    last_epoch = history.iloc[-1].to_dict()\n",
    "    sanitize = {\n",
    "        sanitize_ml_labels(label):value\n",
    "        for label, value in last_epoch.items()\n",
    "        if label not in (\"lr\")\n",
    "    }\n",
    "    training = {\n",
    "        key:val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" not in key\n",
    "    }\n",
    "    validation = {\n",
    "        sanitize_ml_labels(key.replace(\"Val \", \"\")):val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" in key\n",
    "    }\n",
    "\n",
    "    report.append({\n",
    "        \"run\":\"training\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **training\n",
    "    })\n",
    "    report.append({\n",
    "        \"run\":\"validation\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **validation\n",
    "    })\n",
    "\n",
    "report = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training link prediction on some other models\n",
    "Here we execute the link prediction using Random Forests, Decision Trees and Logistic Regression so to have a good comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a3bfe77ec64dce8b88abb588e8b0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Embedding', max=1.0, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "kwargs = {\n",
    "    \"DecisionTreeClassifier\":dict(\n",
    "        max_depth=30,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"RandomForestClassifier\":dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=30,\n",
    "        n_jobs=cpu_count(),\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LogisticRegression\":dict(\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "def metric_report(y_true, y_pred):\n",
    "    metrics = (\n",
    "        roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "    )\n",
    "    return {\n",
    "        sanitize_ml_labels(metric.__name__):metric(y_true, y_pred)\n",
    "        for metric in metrics\n",
    "    }\n",
    "\n",
    "metrics_reports_path = \"classical_link_prediction/linear_models_reports.csv\"\n",
    "if os.path.exists(metrics_reports_path):\n",
    "    metrics_reports = pd.read_csv(metrics_reports_path)\n",
    "else:\n",
    "    metrics_reports = []\n",
    "\n",
    "    for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "        for model_builder in tqdm((DecisionTreeClassifier, RandomForestClassifier, LogisticRegression), desc=\"Model\", leave=False):\n",
    "            model = model_builder(**kwargs[model_builder.__name__])\n",
    "            train_x, train_y = train\n",
    "            valid_x, valid_y = valid\n",
    "            model.fit(train_x, train_y)\n",
    "            train_y_pred = model.predict(train_x)\n",
    "            valid_y_pred = model.predict(valid_x)\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"training\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(train_y, train_y_pred)\n",
    "            })\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"validation\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(valid_y, valid_y_pred)\n",
    "            })\n",
    "\n",
    "    metrics_reports = pd.DataFrame(metrics_reports)\n",
    "    metrics_reports.to_csv(metrics_reports_path, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = pd.concat([\n",
    "    metrics_reports,\n",
    "    report\n",
    "])\n",
    "\n",
    "all_reports.to_csv(\"classical_link_prediction/all_reports.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2969bf12fc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# show_standard_deviation is False because there is only one holdout!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m barplots(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mall_reports\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/barplots/barplots.py\u001b[0m in \u001b[0;36mbarplots\u001b[0;34m(df, groupby, show_standard_deviation, title, data_label, path, sanitize_metrics, use_multiprocessing, verbose, **barplot_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mRead\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbarplot\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     groupby = df.groupby(groupby).agg(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"std\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshow_standard_deviation\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ).sort_index()\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)\u001b[0m\n\u001b[1;32m   5799\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5801\u001b[0;31m         return groupby_generic.DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   5802\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5803\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'run'"
     ]
    }
   ],
   "source": [
    "from barplots import barplots\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# show_standard_deviation is False because there is only one holdout!\n",
    "barplots(\n",
    "    all_reports,\n",
    "    [\"run\", \"method\", \"embedding_model\", \"model\"],\n",
    "    path = 'barplots/{feature}.jpg',\n",
    "    show_standard_deviation=False,\n",
    "    height=5,\n",
    "    subplots=True,\n",
    "    plots_per_row=1\n",
    ")\n",
    "\n",
    "for barplot_path in glob(\"barplots/*\"):\n",
    "    display(Image.open(barplot_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['AUPRC', 'method'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-faec67863da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m scored_per_method = [\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AUPRC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_reports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AUPRC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         self._validate_read_indexer(\n\u001b[0m\u001b[1;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['AUPRC', 'method'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "scored_per_method = [\n",
    "    (group, x[\"AUPRC\"].values)\n",
    "    for group, x in list(all_reports[[\"AUPRC\", \"method\"]].groupby(\"method\"))\n",
    "]\n",
    "\n",
    "for i, (method1, scores1) in enumerate(scored_per_method):\n",
    "    for method2, scores2 in scored_per_method[i+1:]:\n",
    "        print(\n",
    "            method1, method2, wilcoxon(scores1, scores2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
