{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction \n",
    "\n",
    "In this NB, we use the embeddings generated in the NB:\n",
    "`Graph embedding using SkipGram 20201012 homogeneous graph training.ipynb`\n",
    "md5 hash: 261f9f7b0137263728c292a1a878d7baf3f875f3\n",
    "\n",
    "kg-covid-19:\n",
    "version 20201012\n",
    "\n",
    "ensmallen-graph\n",
    "Version: 0.4.4\n",
    "\n",
    "embiggen\n",
    "Version: 0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all files and URLs up top here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "exp_name = \"80_20_kg_covid_19_20201012_training_test_epoch_500_delta_0.0001\"\n",
    "s3_path = \"s3://kg-hub-public-data/embeddings/20201012/\"  # keep trailing slash\n",
    "\n",
    "base_dl_dir = \"downloaded_data\"\n",
    "graph_data_dir = os.path.join(base_dl_dir, \"kg-covid-19-20201012\")\n",
    "embedding_data_dir = os.path.join(base_dl_dir, \"embeddings-20201012\")\n",
    "\n",
    "# graph stuff\n",
    "graph_out_file = os.path.join(graph_data_dir + \"/kg-covid-19.tar.gz\")\n",
    "nodes_file = os.path.join(graph_data_dir, \"merged-kg_nodes.tsv\")\n",
    "edges_file = os.path.join(graph_data_dir, \"merged-kg_edges.tsv\")\n",
    "sorted_edges_file = os.path.join(graph_data_dir, \"merged-kg_edges_SORTED.tsv\")\n",
    "graph_tar_url = \"https://kg-hub.berkeleybop.io/kg-covid-19/20201012/kg-covid-19.tar.gz\"\n",
    "\n",
    "# embeddings URLs\n",
    "base_kghub_url = \"http://kg-hub.berkeleybop.io/\"\n",
    "embeddings_url = os.path.join(base_kghub_url, \"embeddings/20201012/SkipGram_80_20_kg_covid_19_20201012_training_test_epoch_500_delta_0.0001_embedding.npy\")\n",
    "embedding_file = os.path.join(embedding_data_dir, \"SkipGram_embedding.npy\")\n",
    "\n",
    "# params\n",
    "seed = 42\n",
    "train_percentage = 0.8\n",
    "patience = 5\n",
    "min_delta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import get_distribution\n",
    "assert(get_distribution(\"ensmallen-graph\").version == '0.4.4')\n",
    "assert(get_distribution(\"embiggen\").version == '0.6.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the graphs, if necessary\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "os.makedirs(graph_data_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(nodes_file) or not os.path.exists(edges_file):\n",
    "    with urllib.request.urlopen(graph_tar_url) as response, \\\n",
    "        open(graph_out_file, 'wb') as out_file:\n",
    "            data = response.read()  # a `bytes` object\n",
    "            out_file.write(data)\n",
    "    os.system(\"tar -xvzf \" + graph_out_file + \" -C \" + graph_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(embedding_data_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(embedding_file):\n",
    "    with urllib.request.urlopen(embeddings_url) as response, \\\n",
    "        open(embedding_file, 'wb') as out_file:\n",
    "            data = response.read()  # a `bytes` object\n",
    "            out_file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge embeddings\n",
    "We will compute the edge embeddings using all the 5 available methods:\n",
    "\n",
    "- Hadamart: an element-wise product\n",
    "- Mean\n",
    "- Norm L1\n",
    "- Norm L2\n",
    "- Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import AUC, Recall, Precision\n",
    "\n",
    "def build_link_prediction_model(input_shape:int):\n",
    "    model = Sequential([\n",
    "        Input(input_shape),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\",\n",
    "              activity_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.5),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\n",
    "            AUC(curve=\"PR\", name=\"auprc\"),\n",
    "            AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "            Recall(name=\"Recall\"),\n",
    "            Precision(name=\"Precision\"),            \n",
    "            \"accuracy\"\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 931 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "if not os.path.exists(sorted_edges_file):\n",
    "    graph = EnsmallenGraph.from_unsorted_csv(\n",
    "        edge_path = edges_file,\n",
    "        sources_column=\"subject\",\n",
    "        destinations_column=\"object\",\n",
    "        directed=False,\n",
    "        node_path = nodes_file,\n",
    "        nodes_column = 'id',\n",
    "        node_types_column = 'category',\n",
    "        default_node_type = 'biolink:NamedThing'\n",
    "    )\n",
    "\n",
    "    graph.dump_edges(sorted_edges_file,\n",
    "        sources_column=\"subject\",\n",
    "        destinations_column=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directed': 'false',\n",
       " 'has_node_types': 'true',\n",
       " 'singletons': '8314',\n",
       " 'degree_mean': '81.96836406878597',\n",
       " 'unique_node_types_number': '37',\n",
       " 'nodes_number': '377577',\n",
       " 'has_weights': 'false',\n",
       " 'edges_number': '30949369',\n",
       " 'self_loops_number': '481',\n",
       " 'has_edge_types': 'false',\n",
       " 'unique_edge_types_number': '0',\n",
       " 'self_loops_rate': '0.00001554151233261008',\n",
       " 'density': '0.037284237441157525'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    " \n",
    "graph = EnsmallenGraph.from_sorted_csv(\n",
    "    edge_path = sorted_edges_file,\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    nodes_number=377577,  # should be = or > than actual number\n",
    "    edges_number=30949369,   # same ^\n",
    "    node_path = nodes_file,\n",
    "    nodes_column = 'id',\n",
    "    node_types_column = 'category',\n",
    "    default_node_type = 'biolink:NamedThing'\n",
    ")\n",
    "\n",
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining holdouts and tasks data generator\n",
    "We are going to create the same edge embeddings as in the training of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.7 s, sys: 240 ms, total: 53.9 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# to use a given edge_type in positive edges:\n",
    "# install latest ensmallen_graph\n",
    "# use edge_types param (a list of strings) - validation set will be only these edge types\n",
    "pos_training, pos_validation = graph.connected_holdout(train_size=train_percentage, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coherence_check=False\n",
    "if coherence_check:\n",
    "    assert graph.contains(pos_training)\n",
    "    assert graph.contains(pos_validation)\n",
    "    assert (pos_training | pos_validation).contains(graph)\n",
    "    assert graph.contains(pos_training | pos_validation)\n",
    "    assert not pos_training.overlaps(pos_validation)\n",
    "    assert not pos_validation.overlaps(pos_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 1.86 s, total: 3min 3s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To force neg edges to use nodes from a given graph\n",
    "# install latest ensmallen\n",
    "# seed_graph param - upload TSV with this great\n",
    "neg_training, neg_validation = graph.sample_negatives(\n",
    "   random_state=seed,\n",
    "   negatives_number=graph.get_edges_number(),\n",
    "   allow_selfloops=False  # this has been removed in new ensmallen - this will instead be inferred from the graph itself\n",
    ").random_holdout(random_state=seed, train_size=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from embiggen import GraphTransformer, EdgeTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def task_generator(\n",
    "    pos_training:EnsmallenGraph,\n",
    "    pos_validation:EnsmallenGraph,\n",
    "    neg_training:EnsmallenGraph,\n",
    "    neg_validation:EnsmallenGraph,\n",
    "    train_percentage:float=train_percentage,\n",
    "    seed:int=seed\n",
    "):\n",
    "    \"\"\"Create new generator of tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------\n",
    "    pos_training:EnsmallenGraph,\n",
    "        The positive edges of the training graph.\n",
    "    pos_validation:EnsmallenGraph,\n",
    "        The positive edges of the validation graph.\n",
    "    neg_training:EnsmallenGraph,\n",
    "        The negative edges of the training graph.\n",
    "    neg_validation:EnsmallenGraph,\n",
    "        The negative edges of the validation graph.\n",
    "    train_percentage:float=0.8,\n",
    "    seed:int=42\n",
    "\n",
    "    \"\"\"\n",
    "    for path in tqdm(glob(os.path.join(embedding_data_dir, \"*embedding.npy\")), desc=\"Embedding\"):\n",
    "        model_name = os.path.basename(path).split(\"_\")[0]\n",
    "        embedding = np.load(path, allow_pickle=True)\n",
    "        for method in tqdm(EdgeTransformer.methods, desc=\"Methods\", leave=False):\n",
    "\n",
    "            # create graph transformer object to convert graphs into edge embeddings\n",
    "            transformer = GraphTransformer(method)\n",
    "            transformer.fit(embedding) # pass node embeddings to be used to create edge embeddings\n",
    "            train_edges = np.vstack([ # computing edge embeddings for training graph\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_training, neg_training)\n",
    "            ])\n",
    "            valid_edges = np.vstack([ # computing edge embeddings for validation graph\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_validation, neg_validation)\n",
    "            ])\n",
    "            train_labels = np.concatenate([ # make labels for training graph\n",
    "                np.ones(pos_training.get_edges_number()),\n",
    "                np.zeros(neg_training.get_edges_number())\n",
    "            ])\n",
    "            valid_labels = np.concatenate([ # make labels for validation graph\n",
    "                np.ones(pos_validation.get_edges_number()),\n",
    "                np.zeros(neg_validation.get_edges_number())\n",
    "            ])\n",
    "            train_indices = np.arange(0, train_labels.size)\n",
    "            valid_indices = np.arange(0, valid_labels.size)\n",
    "            np.random.shuffle(train_indices) # shuffle to prevent bias caused by ordering of edge labels\n",
    "            np.random.shuffle(valid_indices) # ``   ``\n",
    "            train_edges = train_edges[train_indices]\n",
    "            train_labels = train_labels[train_indices]\n",
    "            valid_edges = valid_edges[valid_indices]\n",
    "            valid_labels = valid_labels[valid_indices]\n",
    "            yield model_name, method, (train_edges, train_labels), (valid_edges, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b0538709434d73b1a6b1d764baf484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Embedding', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b34efb914c4c91888121e9063eb32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Methods', max=4.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "12090/12090 [==============================] - 148s 12ms/step - loss: 0.4510 - auprc: 0.8644 - auroc: 0.8493 - Recall: 0.7918 - Precision: 0.8465 - accuracy: 0.8241 - val_loss: 0.4770 - val_auprc: 0.8771 - val_auroc: 0.8590 - val_Recall: 0.7170 - val_Precision: 0.8896 - val_accuracy: 0.8140\n",
      "Epoch 2/1000\n",
      "12090/12090 [==============================] - 135s 11ms/step - loss: 0.4338 - auprc: 0.8752 - auroc: 0.8562 - Recall: 0.7852 - Precision: 0.8596 - accuracy: 0.8285 - val_loss: 0.4741 - val_auprc: 0.8796 - val_auroc: 0.8608 - val_Recall: 0.7142 - val_Precision: 0.8968 - val_accuracy: 0.8160\n",
      "Epoch 3/1000\n",
      "12090/12090 [==============================] - 137s 11ms/step - loss: 0.4311 - auprc: 0.8769 - auroc: 0.8573 - Recall: 0.7835 - Precision: 0.8632 - accuracy: 0.8296 - val_loss: 0.4678 - val_auprc: 0.8804 - val_auroc: 0.8613 - val_Recall: 0.7164 - val_Precision: 0.8984 - val_accuracy: 0.8177\n",
      "Epoch 4/1000\n",
      "12090/12090 [==============================] - 134s 11ms/step - loss: 0.4300 - auprc: 0.8774 - auroc: 0.8577 - Recall: 0.7827 - Precision: 0.8648 - accuracy: 0.8301 - val_loss: 0.4698 - val_auprc: 0.8808 - val_auroc: 0.8617 - val_Recall: 0.7199 - val_Precision: 0.8989 - val_accuracy: 0.8195\n",
      "Epoch 5/1000\n",
      "12090/12090 [==============================] - 136s 11ms/step - loss: 0.4295 - auprc: 0.8778 - auroc: 0.8579 - Recall: 0.7821 - Precision: 0.8658 - accuracy: 0.8304 - val_loss: 0.4651 - val_auprc: 0.8809 - val_auroc: 0.8617 - val_Recall: 0.7240 - val_Precision: 0.8981 - val_accuracy: 0.8209\n",
      "Epoch 6/1000\n",
      "12090/12090 [==============================] - 134s 11ms/step - loss: 0.4290 - auprc: 0.8780 - auroc: 0.8581 - Recall: 0.7817 - Precision: 0.8665 - accuracy: 0.8307 - val_loss: 0.4717 - val_auprc: 0.8813 - val_auroc: 0.8620 - val_Recall: 0.7110 - val_Precision: 0.9029 - val_accuracy: 0.8173\n",
      "Epoch 7/1000\n",
      "12090/12090 [==============================] - 134s 11ms/step - loss: 0.4287 - auprc: 0.8781 - auroc: 0.8582 - Recall: 0.7816 - Precision: 0.8670 - accuracy: 0.8308 - val_loss: 0.4662 - val_auprc: 0.8813 - val_auroc: 0.8620 - val_Recall: 0.7164 - val_Precision: 0.9015 - val_accuracy: 0.8191\n",
      "Epoch 8/1000\n",
      "12090/12090 [==============================] - 135s 11ms/step - loss: 0.4285 - auprc: 0.8783 - auroc: 0.8583 - Recall: 0.7815 - Precision: 0.8672 - accuracy: 0.8309 - val_loss: 0.4694 - val_auprc: 0.8816 - val_auroc: 0.8622 - val_Recall: 0.7089 - val_Precision: 0.9038 - val_accuracy: 0.8167\n",
      "Epoch 9/1000\n",
      "12090/12090 [==============================] - 134s 11ms/step - loss: 0.4283 - auprc: 0.8783 - auroc: 0.8584 - Recall: 0.7814 - Precision: 0.8675 - accuracy: 0.8310 - val_loss: 0.4666 - val_auprc: 0.8816 - val_auroc: 0.8622 - val_Recall: 0.7159 - val_Precision: 0.9022 - val_accuracy: 0.8191\n",
      "Epoch 10/1000\n",
      "12090/12090 [==============================] - 135s 11ms/step - loss: 0.4281 - auprc: 0.8784 - auroc: 0.8584 - Recall: 0.7813 - Precision: 0.8678 - accuracy: 0.8312 - val_loss: 0.4656 - val_auprc: 0.8816 - val_auroc: 0.8622 - val_Recall: 0.7175 - val_Precision: 0.9015 - val_accuracy: 0.8196\n",
      "Epoch 1/1000\n",
      "12090/12090 [==============================] - 146s 12ms/step - loss: 0.4229 - auprc: 0.8936 - auroc: 0.8700 - Recall: 0.7961 - Precision: 0.8477 - accuracy: 0.8266 - val_loss: 0.4206 - val_auprc: 0.9064 - val_auroc: 0.8853 - val_Recall: 0.8064 - val_Precision: 0.8472 - val_accuracy: 0.8305\n",
      "Epoch 2/1000\n",
      "11242/12090 [==========================>...] - ETA: 8s - loss: 0.4062 - auprc: 0.9051 - auroc: 0.8836 - Recall: 0.8019 - Precision: 0.8468 - accuracy: 0.8284"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from notipy_me import Notipy, KerasNotipy\n",
    "\n",
    "histories = {}\n",
    "strategy = MirroredStrategy()\n",
    "os.makedirs(\"classical_link_prediction\", exist_ok=True)\n",
    "\n",
    "for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "    history_path = f\"classical_link_prediction/{embedding_model}_{method}.csv\"\n",
    "    if os.path.exists(history_path):\n",
    "        histories[(embedding_model, method)] = pd.read_csv(history_path)\n",
    "        continue\n",
    "    with strategy.scope():\n",
    "        model = build_link_prediction_model(train[0].shape[1:])\n",
    "        \n",
    "        history = pd.DataFrame(model.fit(\n",
    "            *train,\n",
    "            batch_size=2**12,\n",
    "            validation_data=valid,\n",
    "            epochs=1000,\n",
    "            callbacks=[\n",
    "                EarlyStopping(\"val_loss\", patience=patience, min_delta=min_delta),\n",
    "                ReduceLROnPlateau(),\n",
    "                # KerasNotipy(task_name=\"LR 20201012\")\n",
    "            ]\n",
    "        ).history)\n",
    "\n",
    "        history.to_csv(history_path, index=False)\n",
    "        histories[(embedding_model, method)] = history\n",
    "        \n",
    "        h5_file = f\"{embedding_model}_{exp_name}_{method}_finalized_model.h5\"\n",
    "        model.save(h5_file)\n",
    "        os.system(f\"s3cmd put --acl-public --cf-invalidate {h5_file} {s3_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all the computer histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "\n",
    "for history in histories.values():\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of various embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we covert the histories into an homogeneous report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "\n",
    "report = []\n",
    "for (model, method), history in histories.items():\n",
    "    last_epoch = history.iloc[-1].to_dict()\n",
    "    sanitize = {\n",
    "        sanitize_ml_labels(label):value\n",
    "        for label, value in last_epoch.items()\n",
    "        if label not in (\"lr\")\n",
    "    }\n",
    "    training = {\n",
    "        key:val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" not in key\n",
    "    }\n",
    "    validation = {\n",
    "        sanitize_ml_labels(key.replace(\"Val \", \"\")):val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" in key\n",
    "    }\n",
    "\n",
    "    report.append({\n",
    "        \"run\":\"training\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **training\n",
    "    })\n",
    "    report.append({\n",
    "        \"run\":\"validation\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **validation\n",
    "    })\n",
    "\n",
    "report = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training link prediction on some other models\n",
    "Here we execute the link prediction using Random Forests, Decision Trees and Logistic Regression so to have a good comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "kwargs = {\n",
    "    \"DecisionTreeClassifier\":dict(\n",
    "        max_depth=30,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"RandomForestClassifier\":dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=30,\n",
    "        n_jobs=cpu_count(),\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LogisticRegression\":dict(\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "def metric_report(y_true, y_pred):\n",
    "    metrics = (\n",
    "        roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "    )\n",
    "    return {\n",
    "        sanitize_ml_labels(metric.__name__):metric(y_true, y_pred)\n",
    "        for metric in metrics\n",
    "    }\n",
    "\n",
    "metrics_reports_path = \"classical_link_prediction/linear_models_reports.csv\"\n",
    "if os.path.exists(metrics_reports_path):\n",
    "    metrics_reports = pd.read_csv(metrics_reports_path)\n",
    "else:\n",
    "    metrics_reports = []\n",
    "\n",
    "    for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "        for model_builder in tqdm((DecisionTreeClassifier, RandomForestClassifier, LogisticRegression), desc=\"Model\", leave=False):\n",
    "            model = model_builder(**kwargs[model_builder.__name__])\n",
    "            train_x, train_y = train\n",
    "            valid_x, valid_y = valid\n",
    "            model.fit(train_x, train_y)\n",
    "            train_y_pred = model.predict(train_x)\n",
    "            valid_y_pred = model.predict(valid_x)\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"training\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(train_y, train_y_pred)\n",
    "            })\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"validation\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(valid_y, valid_y_pred)\n",
    "            })\n",
    "\n",
    "    metrics_reports = pd.DataFrame(metrics_reports)\n",
    "    metrics_reports.to_csv(metrics_reports_path, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = pd.concat([\n",
    "    metrics_reports,\n",
    "    report\n",
    "])\n",
    "\n",
    "all_reports.to_csv(\"classical_link_prediction/all_reports.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from barplots import barplots\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# show_standard_deviation is False because there is only one holdout!\n",
    "barplots(\n",
    "    all_reports,\n",
    "    [\"run\", \"method\", \"embedding_model\", \"model\"],\n",
    "    path = 'barplots/{feature}.jpg',\n",
    "    show_standard_deviation=False,\n",
    "    height=5,\n",
    "    subplots=True,\n",
    "    plots_per_row=1\n",
    ")\n",
    "\n",
    "for barplot_path in glob(\"barplots/*\"):\n",
    "    display(Image.open(barplot_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "scored_per_method = [\n",
    "    (group, x[\"AUPRC\"].values)\n",
    "    for group, x in list(all_reports[[\"AUPRC\", \"method\"]].groupby(\"method\"))\n",
    "]\n",
    "\n",
    "for i, (method1, scores1) in enumerate(scored_per_method):\n",
    "    for method2, scores2 in scored_per_method[i+1:]:\n",
    "        print(\n",
    "            method1, method2, wilcoxon(scores1, scores2)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
